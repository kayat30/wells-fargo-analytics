{"name":"Wells Fargo Analytics Competition","tagline":"","body":"![](https://www.facebook.com/photo.php?fbid=906609749371004&l=df96812d98)\r\n\r\n\r\n# Wells Fargo Analytics Competition\r\n\r\n\r\n##Background\r\n\r\nWells Fargo challenged data scientists to extract information from social media posts mentioning banks. They provided tweets and Facebook posts that included references to four banks (BankA, BankB, BankC and BankD) in order to determine what financial and bank topics consumers posted about in social media. They were also interested in the reasons consumers post about banks and whether the substance of the posts is consistent across the industry or isolated to individual banks. Specifically, they challenged data scientists to develop an approach that identifies, classifies, and extracts the underlying drivers in social media data. \r\n\r\n[Competition Summary](http://innercircle.engineering.asu.edu/wp-content/uploads/2015/11/Wells-Fargo-Campus-Analytic-Challenge-Information.pdf)\r\n\r\n##Approach\r\n\r\nAnalysis flowchart\r\n![](http://i.imgur.com/Dq2crEL.png)\r\n\r\n###The Data\r\n\r\n\r\nOriginal text file  \r\n\r\n![](http://i.imgur.com/erexpCj.png)\r\n\r\nContent of posts  \r\n\r\n![](http://i.imgur.com/qprA8vl.png)\r\n\r\nThe first step in the analytic process was to clean/preprocess the data. \r\n\r\n```R\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n#test 10,000 texts: good sample size\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))   \r\n\r\n#REMEMBER: IT MATTERS THE ORDER IN WHICH TM_MAP EXPRESSIONS ARE RUN\r\nlibrary(rJava)\r\ndocs <- tm_map(docs, content_transformer(tolower)) # convert to lowercase first\r\ndocs <- tm_map(docs, removeWords, stopwords('english'))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind = \"SMART\"))\r\n#metaData/recurrent words\r\nmyMeta <- c(\"name\",\"bank\",\"banka\",\"bankb\",\"bankc\", \"bankd\", \"banke\",\r\n            \"internet\",\"https\", \"rettwit\", \"twit_hndl_banka\",\r\n            \"twit_hndl_bankb\",\"twit_hndl_bankc\",\"twit_hndl_bankd\", \"phone\",\r\n            \"dirmsg\", \"dir_msg\", \"street\",\"name_resp\",\"bankds\", \"and\", \"for\", \"the\", \"you\",\r\n            \"twithndlbanka\",\"dirmsg\",\"ret_twit\",\"twit_hndl\")\r\ndocs <- tm_map(docs, removeWords, myMeta)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\nnew.df <-data.frame(text=unlist(sapply(docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\n```\r\n\r\n###Frequency Analysis\r\n\r\nAfter cleaning the data, analyzed frequent terms in posts to get a sense of what topics consumers most frequently posted about.\r\n\r\n```R\r\ndtm <- DocumentTermMatrix(docs)\r\ndtm = removeSparseTerms(dtm, 0.98)\r\nfindFreqTerms(dtm,200)\r\n\r\nfreq <- colSums(as.matrix(dtm))  \r\nfreq\r\nord <- order(freq)   \r\n\r\nlibrary(wordcloud)\r\nwordcloud(names(freq), freq, colors=brewer.pal(8, \"Dark2\"))\r\n```\r\n\r\nWord cloud of all posts  \r\n\r\n![](http://i.imgur.com/mOsebx5.png)\r\n\r\nAlso generated word clouds for each bank \r\n\r\n```R\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\n##pass these in as argument in dtm <- DocumentTermMatrix(docs)\r\n##repeat word cloud procedure \r\n```\r\n\r\nBank A  \r\n\r\n![](http://i.imgur.com/qyEGLEi.png)\r\n\r\nBank B  \r\n\r\n![](http://i.imgur.com/CH2lx1P.png)\r\n\r\nBank C  \r\n\r\n![](http://i.imgur.com/KwXPfas.png)\r\n\r\nBank D  \r\n\r\n![](http://i.imgur.com/qkAmlR3.png)\r\n\r\n###Sentiment Analysis\r\n\r\nAnalyzed sentiment of posts to identify what drives consumers to post about banks (dissatisfaction, appreciation, etc).\r\n\r\nFirst, used function to match words in posts with list of positive and negative words\r\n\r\n```R\r\n# Based on: http://www.ihub.co.ke/blogs/23216\r\n# Download and upload: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\r\n#system('unrar e opinion-lexicon-English.rar')\r\n\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  # we got a vector of sentences. plyr will handle a list\r\n  # or a vector as an \"l\" for us\r\n  # we want a simple array (\"a\") of scores back, so we use \r\n  # \"l\" + \"a\" + \"ply\" = \"laply\":\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    # and convert to lower case:\r\n    sentence = tolower(sentence)\r\n    \r\n    # split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    # we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n```\r\n\r\nThen made subsets of the data based on posts mentioning each bank, names (redacted in text file to NAME), and internet addresses (redacted in text file to INTERNET)\r\n\r\n```R\r\ndf.1000$FullText = as.character(df.1000$FullText)\r\ndf.bankA = df.1000[bankA.idx,]\r\ndf.bankB = df.1000[bankB.idx,]\r\ndf.bankC = df.1000[bankC.idx,]\r\ndf.bankD = df.1000[bankD.idx,]\r\ndf.names = df.1000[name.idx,]\r\ndf.internet = df.1000[internet.idx,]\r\n```\r\n\r\nNext, calculated sentiment score for each.\r\n\r\n```R\r\ndf.sent = df.BankA ##or whichever subset to analyze\r\n\r\nscores = score.sentiment(df.sent$FullText, pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\nscores$pos = as.numeric(scores$score >= 1)\r\nscores$neg = as.numeric(scores$score >= -1)\r\n\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\n```\r\n\r\nThe global scores (a rough measure of general sentiment) are as follows:\r\n\r\nAll Banks (df.1000): 59\r\n\r\nBank A: 57\r\n\r\nBank B: 56\r\n\r\nBank C: 74\r\n\r\nBank D: 50\r\n\r\nText with “NAME”: 50\r\n\r\nText with “INTERNET”: 55  \r\n\r\nIt is worth being noted that Bank C had a much higher sentiment score than the average.\r\n\r\nNext, plots of sentiment scores were generated, separating by media type (Twitter or Facebook). These plots can be found at the bottom of the page.\r\n\r\n```R\r\nscores$mediatype = df.sent$MediaType\r\n\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\n  geom_boxplot(aes(fill=mediatype)) +\r\n  scale_fill_manual(values=cols) +\r\n  geom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\n  labs(title = \"Media Type's Sentiment Scores: All Banks\") + \r\n  xlab('Media Type') + ylab('Sentiment Score')\r\n\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\n\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Sentiment Score: All Banks\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\nggplot(mediatype_pos, aes(x = factor(mediatype), y = mean_pos, fill=mediatype)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Positive Sentiment Score: All Banks\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_neg$mediatype, mediatype_neg$mean_neg)\r\n\r\nggplot(mediatype_neg, aes(x = factor(mediatype), y = mean_neg, fill=mediatype)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Negative Sentiment Score: All Banks\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n```\r\n\r\nTo obtain the context of the rest of the content of the posts with very positive and very negative scores (>1 and <-1 respectively), word clouds of very positive and very negative posts were generated. These word clouds can be found at the bottom of the page.\r\n\r\n```R\r\nlibrary(tm)\r\nlibrary(wordcloud)\r\n\r\ndf.sent = new.df.BankA\r\n\r\nscores = score.sentiment(df.sent$text, pos, neg, .progress='text')\r\n\r\nscores$text = as.character(scores$text)\r\n\r\nposIndices = which(as.numeric(scores$score) > 1)\r\nnegIndices = which(as.numeric(scores$score) < -1)\r\n\r\nposPosts = scores[posIndices,2]\r\nnegPosts = scores[negIndices,2]\r\n\r\nwordcloud(negPosts, scale = c(3, 0.0), min.freq = 20, colors=brewer.pal(8, \"Dark2\"))\r\n```\r\n\r\n##Conclusions\r\n\r\nBased on the graphs and word clouds (found at the bottom of the page in the Graphs section), a few insights can be made. \r\n\r\nFirst, based on the sentiment analysis, the data shows that people are more likely to post more extreme content on Facebook rather than Twitter. The general trend in the data was that average very positive and very negative scores were much higher for Facebook than for Twitter. This finding could have implications as to how banks market themselves on each social medium.\r\n\r\nBased on the word frequency analysis and sentiment-based frequency analysis, the data reveals several trends in consumer topics. In the word clouds, in which larger words are those that appear more frequently in posts, much of the content of these posts were directly related to banking (\"money\", \"card\", \"center\", \"account\", \"financial\", \"credit\" for example). \r\nIn the sentiment-based word clouds, there was a common theme in positive clouds of content relating to banks' customer service, support, and values (\"service\", \"free\", \"people\", \"confidence\", \"personal\", \"support\", \"team\", \"customer\", \"mission\"). In the negative clouds, the content was also customer service (related to call centers and time), but more heavily banking services (\"customer\", \"service\", \"time\", \"call\", \"money\", \"charges\", \"pay\", \"card\", \"account\", \"fraud\", \"managers\", \"unable\"). This finding could have implications as to how banks conduct their services, especially customer service, but also ease of the banking process. \r\n\r\nOne very relevant takeaway would be the prevalence of dissatisfaction with call centers. This suggests that it is increasingly important for banks to properly train those employees in the highest standard of customer service. Banks should consider expanding their budgets for this sort of training since dissatisfaction can be broadcast to social platforms and tarnish the image of the bank as well as threatening the future involvement of current clientele.\r\n\r\n##Graphs\r\n\r\n![](http://i.imgur.com/KbZVGQg.png) ![](http://i.imgur.com/2A7iiGh.png) \r\n![](http://i.imgur.com/Fe5NBmo.png) ![](http://i.imgur.com/iodCywR.png)\r\n\r\n![](http://i.imgur.com/HiNabnK.png) ![](http://i.imgur.com/iXqS4L7.png) \r\n![](http://i.imgur.com/OOQYs6v.png) ![](http://i.imgur.com/7RTMJX1.png)\r\n\r\n![](http://i.imgur.com/FE9Q3FZ.png) ![](http://i.imgur.com/XxvUpct.png)\r\n![](http://i.imgur.com/pcGglih.png) ![](http://i.imgur.com/7o5WC1K.png)\r\n\r\n![](http://i.imgur.com/5t8Kqrc.png) ![](http://i.imgur.com/Nr8EJCb.png)\r\n![](http://i.imgur.com/9eJqAee.png) ![](http://i.imgur.com/VKApV17.png)\r\n\r\n![](http://i.imgur.com/oknPYyU.png) ![](http://i.imgur.com/7Y2mF7Q.png)\r\n![](http://i.imgur.com/niYXz6g.png) ![](http://i.imgur.com/4Jrrdf5.png)\r\n\r\n![](http://i.imgur.com/4NXYVh8.png)\r\n\r\n![](http://i.imgur.com/QxAFXiy.png)\r\n\r\nBank A very positive  \r\n\r\n![](http://i.imgur.com/omMIvrO.png) \r\n\r\nBank A very negative  \r\n\r\n![](http://i.imgur.com/2BFyP4d.png)\r\n\r\nBank B very positive  \r\n\r\n![](http://i.imgur.com/2LW8p5T.png) \r\n\r\nBank B very negative  \r\n\r\n![](http://i.imgur.com/CU3hrzK.png)\r\n\r\nBank C very positive  \r\n\r\n![](http://i.imgur.com/s29D7Rm.png) \r\n\r\nBank C very negative  \r\n\r\n![](http://i.imgur.com/UBxIBmr.png)\r\n\r\nBank D very positive  \r\n\r\n![](http://i.imgur.com/FrOhnNz.png) \r\n\r\nBank D very negative  \r\n\r\n![](http://i.imgur.com/inonpvJ.png)\r\n\r\nINTERNET very positive  \r\n\r\n![](http://i.imgur.com/PuRVS9F.png)\r\n\r\nINTERNET very negative  \r\n\r\n![](http://i.imgur.com/2RziLyu.png)\r\n\r\nNAME very positive  \r\n\r\n![](http://i.imgur.com/bJf3Ujw.png) \r\n\r\nNAME very negative  \r\n\r\n![](http://i.imgur.com/3taSp6k.png)\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}