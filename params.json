{"name":"Wells Fargo Analytics Competition","tagline":"","body":"# Wells Fargo Analytics Competition\r\n\r\n##Background\r\n\r\nWells Fargo challenged data scientists to extract information from social media posts mentioning banks. They provided tweets and Facebook posts that included references to four banks (BankA, BankB, BankC and BankD) in order to determine what financial and bank topics consumers posted about in social media. They were also interested in the reasons consumers post about banks and whether the substance of the posts is consistent across the industry or isolated to individual banks. Specifically, they challenged data scientists to develop an approach that identifies, classifies, and extracts the underlying drivers in social media data. \r\n\r\n[Competition Summary](http://innercircle.engineering.asu.edu/wp-content/uploads/2015/11/Wells-Fargo-Campus-Analytic-Challenge-Information.pdf)\r\n\r\n##Approach\r\n\r\n###The Data\r\n\r\n![Original text file](http://imgur.com/erexpCj)\r\n\r\n![Content of posts](http://imgur.com/qprA8vl)\r\n\r\nThe first step in the analytic process was to clean/preprocess the data. \r\n\r\n```R\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n#test 10,000 texts: good sample size\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))   \r\n\r\n#REMEMBER: IT MATTERS THE ORDER IN WHICH TM_MAP EXPRESSIONS ARE RUN\r\nlibrary(rJava)\r\ndocs <- tm_map(docs, content_transformer(tolower)) # convert to lowercase first\r\ndocs <- tm_map(docs, removeWords, stopwords('english'))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind = \"SMART\"))\r\n#metaData/recurrent words\r\nmyMeta <- c(\"name\",\"bank\",\"banka\",\"bankb\",\"bankc\", \"bankd\", \"banke\",\r\n            \"internet\",\"https\", \"rettwit\", \"twit_hndl_banka\",\r\n            \"twit_hndl_bankb\",\"twit_hndl_bankc\",\"twit_hndl_bankd\", \"phone\",\r\n            \"dirmsg\", \"dir_msg\", \"street\",\"name_resp\",\"bankds\", \"and\", \"for\", \"the\", \"you\",\r\n            \"twithndlbanka\",\"dirmsg\",\"ret_twit\",\"twit_hndl\")\r\ndocs <- tm_map(docs, removeWords, myMeta)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\nnew.df <-data.frame(text=unlist(sapply(docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\n```\r\n\r\n###Frequency Analysis\r\n\r\nAfter cleaning the data, analyzed frequent terms in posts to get a sense of what topics consumers most frequently posted about.\r\n\r\n```R\r\ndtm <- DocumentTermMatrix(docs)\r\ndtm = removeSparseTerms(dtm, 0.98)\r\nfindFreqTerms(dtm,200)\r\n\r\nfreq <- colSums(as.matrix(dtm))  \r\nfreq\r\nord <- order(freq)   \r\n\r\nlibrary(wordcloud)\r\nwordcloud(names(freq), freq, colors=brewer.pal(8, \"Dark2\"))\r\n```\r\n\r\n![Word cloud of all posts](http://imgur.com/mOsebx5)\r\n\r\nAlso generated word clouds for each bank \r\n\r\n```R\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\n##pass these in as argument in dtm <- DocumentTermMatrix(docs)\r\n##repeat word cloud procedure \r\n```\r\n\r\n![Bank A](http://imgur.com/qyEGLEi)\r\n![Bank B](http://imgur.com/CH2lx1P)\r\n![Bank C](http://imgur.com/KwXPfas)\r\n![Bank D](http://imgur.com/qkAmlR3)\r\n\r\n###Sentiment Analysis\r\n\r\nAnalyzed sentiment of posts to identify what drives consumers to post about banks (dissatisfaction, appreciation, etc).\r\n\r\nFirst, used function to match words in posts with list of positive and negative words\r\n\r\n```R\r\n# Based on: http://www.ihub.co.ke/blogs/23216\r\n# Download and upload: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\r\n#system('unrar e opinion-lexicon-English.rar')\r\n\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  # we got a vector of sentences. plyr will handle a list\r\n  # or a vector as an \"l\" for us\r\n  # we want a simple array (\"a\") of scores back, so we use \r\n  # \"l\" + \"a\" + \"ply\" = \"laply\":\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    # and convert to lower case:\r\n    sentence = tolower(sentence)\r\n    \r\n    # split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    # we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n```\r\n\r\nThen made subsets of the data based on posts mentioning each bank, names (redacted in text file to NAME), and internet addresses (redacted in text file to INTERNET)\r\n\r\n```R\r\ndf.1000$FullText = as.character(df.1000$FullText)\r\ndf.bankA = df.1000[bankA.idx,]\r\ndf.bankB = df.1000[bankB.idx,]\r\ndf.bankC = df.1000[bankC.idx,]\r\ndf.bankD = df.1000[bankD.idx,]\r\ndf.names = df.1000[name.idx,]\r\ndf.internet = df.1000[internet.idx,]\r\n```\r\n\r\nNext, calculated sentiment score for each.\r\n\r\n```R\r\ndf.sent = df.BankA ##or whichever subset to analyze\r\n\r\nscores = score.sentiment(df.sent$FullText, pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\nscores$pos = as.numeric(scores$score >= 1)\r\nscores$neg = as.numeric(scores$score >= -1)\r\n\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\n```\r\n\r\nThe global scores (a rough measure of general sentiment) are as follows:\r\n\r\nAll Banks (df.1000): 59\r\nBank A: 57\r\nBank B: 56\r\nBank C: 74\r\nBank D: 50\r\nText with “NAME”: 50\r\nText with “INTERNET”: 55\r\n\r\nNext, plots of sentiment scores were generated, separating by media type (Twitter or Facebook).\r\n\r\n```R\r\nscores$mediatype = df.sent$MediaType\r\n\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\n  geom_boxplot(aes(fill=mediatype)) +\r\n  scale_fill_manual(values=cols) +\r\n  geom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\n  labs(title = \"Media Type's Sentiment Scores: All Banks\") + \r\n  xlab('Media Type') + ylab('Sentiment Score')\r\n\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\n\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Sentiment Score: All Banks\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\nggplot(mediatype_pos, aes(x = factor(mediatype), y = mean_pos, fill=mediatype)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Positive Sentiment Score: All Banks\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_neg$mediatype, mediatype_neg$mean_neg)\r\n\r\nggplot(mediatype_neg, aes(x = factor(mediatype), y = mean_neg, fill=mediatype)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Negative Sentiment Score: All Banks\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n```\r\n\r\n\r\n\r\n\r\n\r\nEach submission will contain a written report with the following sections:\r\n• Deliverable A: Describe your Approach and Methodology, including a visual representation of analytic process flow\r\n• Deliverable B: Discuss the data and its relationship to social conversation drivers\r\n• Deliverable C: Document your code and reference the analytic process flow-diagram from deliverable A\r\n• Deliverable D: Create list of topics and substance you found\r\n• Deliverable E: Create narrative of insights supported by quantitative results (should include graphs or charts)\r\n\r\n\r\n\r\n##Conclusions\r\n\r\n\r\n\r\n\r\n```R\r\nloading(texts)\r\n```\r\n\r\n![](http://i.imgur.com/oGyY7Zo.jpg)\r\n\r\n\r\n\r\n## Important Links\r\n* [Learn2Mine - where you complete assignments](http://learn2mine.appspot.com/)\r\n* [Join Learn2Mine Class](http://learn2mine.appspot.com/EnrollClass?key=G5avwmEBUIdo1UTFDGCX)\r\n* [RStudio - where you code](http://freyja.cs.cofc.edu/rstudio-learn2mine)\r\n* [Course Materials](https://www.dropbox.com/sh/u7rd31jqtkfv9la/AAA85S3odhJ28VrRgkBrNdUia?dl=0)\r\n* [Schedule, news, and assignments. You must check this regularly](https://docs.google.com/presentation/d/1e0AdOUa3nbjkkV-j4ZcG8acG_INjmv42nPkpr5xuGFc/edit?usp=sharing)\r\n\r\n# Syllabus\r\n## Official Course Description\r\n<p align=\"justify\">\r\n<a href=\"http://www.cofc.edu\"><img src=\"http://freyja.cs.cofc.edu/cofc.sepng\" height=\"50\" align=\"right\" hspace=\"10px\"> </a>\r\nIntroduction to the use of computer based tools for the analysis of large data sets for the purpose of knowledge discovery. Students will learn to understand the Data Science process and the difference between deductive hypothesis-driven and inductive data-driven modeling. Students will have hands-on experience with various on-line analytical processing and data mining software and complete a project using real data.\r\n</p>\r\n\r\n## Our plan\r\nThe plan this semester is to work through the analysis of real world datasets provided by the data science website called kaggle.com and from my own personal research. The class will be heavily structured around these projects. In fact, there will be only two examinations (1 midterm and 1 final). These will be on the theory of data science and the algorithms covered throughout class. The most important component of the course will be the practical experience gained as a data scientist, and this will be built upon by exploring:\r\n\r\nTargeted data science lessons on concepts discussed in class. The topics will vary from week to week. They are designed to build up your skills to accomplish the next two tasks.\r\nPredicting the survival rate on the titanic (https://www.kaggle.com/c/titanic-gettingStarted). This is the first data science competition that we will do as a class. As we progress through the class, there will be a number of assignments focused on this prediction task.\r\nBioinformatics: Next-generation genomics data analysis\r\nEach week will have a consistent schedule. The first class will be traditional lecture style with a heavy emphasis on interactive discussion, where I will go over the theory behind the algorithms and concepts. The second class will be mostly lab style. My goal is to be your guide as you gain experience being a data scientist. Though this time will also be used to teach you the R programming language as well. It is during this second class and out of the classroom that you will gain practical experience as a data scientist. If you have laptops, please bring them to the second class of each week.\r\n\r\n## Course Details\r\n\r\n### Contact Information\r\n\r\nProfessor: Dr. Paul Anderson\r\nOffice: 313 Harbor Walk East\r\nOffice Hours: My door is always open. Even if it isn't, please knock. I always love to hear from students. I have a little sign that I try to keep up in the window to show when I am in the office. Tuesday and Thursday from 2:00 - 3:30 PM are my official hours.\r\nE-mail: andersonpe2@cofc.edu\r\nOffice Phone: 953-8151\r\nFacebook: andersonpe2@cofc.edu\r\nFacebook group: https://www.facebook.com/groups/1689269657959552/\r\n\r\n### Course Times\r\nSection 01 - TR 09:55 am-11:10 am in HWEA 334\r\n\r\nSection 02 - TR\t03:35 pm-04:50 pm in HWEA 300\r\n\r\n### Course (learning) outcomes\r\n\r\nTo gain an overview the field of knowledge discovery\r\nTo be able to distinguish and translate between data, information, and knowledge\r\nTo learn how to store, query, aggregate data in databases\r\nTo be able to distinguish problems based on computability\r\nTo learn how to implement distributed computing and storage\r\nTo apply algorithms for inductive and deductive reasoning\r\nTo learn introductory and state-of-the-art data mining algorithms\r\nTo apply data mining, statistical inference, and machine learning algorithms to a variety of datasets, including text, image, biological, and health\r\nTo apply information filtering on real world datasets\r\nTo apply information validation on real world datasets\r\nTo apply artificial intelligence concepts to real world datasets\r\nTo understand the social, ethical, and legal issues of informatics and data science\r\n\r\n### Grading Policy\r\n\r\nMidterm - 15%\r\nFinal Exam - 15%\r\nHomework - 10%\r\nProgramming Assignments and Final Project - 60%\r\nGrading Scale: A: 90-100; B: 80-89; C: 70-79; D: 65-69; F: <65. Plusses and minuses will be used at the discretion of the instructor.\r\n\r\nGrading Guidelines: Submitted work requires Analysis, Evaluation, and Creation of ideas, concepts, and materials into various deliverables (e.g., see revised Bloom's Taxonomy and reference below).\r\n\r\nThe grade of A is for work that involves high-quality achievement in all three Bloom areas.\r\nThe grade of B is for work that involves high-quality achievement in at least two Bloom areas, and medium-level achievement in the other.\r\nThe grade of C is for work that involves high-quality achievement in at least one Bloom area, and medium-level achievement in the others.\r\nThe grade of F is for work that does not meet above criteria.\r\nReference: Errol Thompson, Andrew Luxton-Reilly, Jacqueline L. Whalley, Minjie Hu, and Phil Robbins. 2008. Bloom's taxonomy for CS assessment. In Proceedings of the tenth conference on Australasian computing education - Volume 78 (ACE '08), Simon Hamilton and Margaret Hamilton (Eds.), Vol. 78. Australian Computer Society, Inc., Darlinghurst, Australia, Australia, 155-161.\r\n\r\n### Homework Policy\r\n\r\nWritten homework will placed under my office door by 5 PM on the due date. No late homework will be accepted. Cheating/sharing will result in a zero on the assignment and a report to the judicial board.\r\n\r\n### Programming Assignments\r\n\r\nProgramming assignments will be submitted through the Learn2Mine environment. There will be a combination of in-class lab assignments, and out of programming assignments.\r\n\r\n### Honor Code\r\n\r\nYou must do your work alone (or with your teammates, for group assignments).\r\nYou must identify your sources of material and inspiration. It is a violation of the honor code to present someone else's work or ideas as your own.\r\nIn any course deliverable, you must always identify the person(s) that helped you (directly or indirectly), if any, and explain their contribution to your work.\r\nAlso see the College of Charleston Student Handbook, especially sections on The Honor Code (p. 11), and Student Code of Conduct (p. 12). There is other useful information there.\r\nClassroom Policies\r\n\r\nYou are expected to take good notes during class.\r\nYou are expected to participate in class with questions and invited discussion.\r\nYou are expected to attend all classes. The grade 'WA' will be given for excessive (>= 3) absences. If you miss class, you must get an absence memo from the Associate Dean of Students Office; also, you are responsible for announcements made in class, assignment due dates, etc.\r\nYou should turn off all electronic devices (e.g., cell phones, pagers, etc.).\r\nIn summary, you should contribute positively to the classroom learning experience, and respect your classmates right to learn (see College of Charleston Student Handbook, section on Classroom Code of Conduct (p. 58)).\r\nLate Policy\r\n\r\nNo late days will be allowed without an excuse. Falling behind on assignments will make it difficult to achieve the learning outcomes of this course.\r\n\r\n### Facebook Group\r\n\r\nYou are required to join the Facebook Group. The majority of class related discussions will be carried out in this forum.\r\n\r\n[Join the Facebook group here](https://www.facebook.com/groups/1689269657959552/)\r\n\r\n## Midterm\r\n\r\nThe is scheduled for Tuesday, October 27th\r\n\r\n## Final\r\n\r\nThe final exam will be take home.\r\n\r\nThe final exam time will be used for final project presentations.\r\n\r\nSection 01 - Saturday, December 12th from 8 - 11 AM\r\n\r\nSection 02 - Thursday, December 10th from 4 - 7 PM\r\n\r\n# <a href=\"http://anderson-lab.github.io/\">Anderson Data Science Research Lab</a>\r\n\r\n<p align=\"justify\">\r\n<a href=\"http://anderson-lab.github.io/\"><img src=\"http://freyja.cs.cofc.edu/Paul-labs-logo.png\" alt=\"Data Science Research Lab\" height=\"100\" align=\"right\"  hspace=\"10px\"/></a>\r\nThe Anderson Data Science Research Lab specializes in applying data mining, machine learning, and artificial intelligence to the fields of bioinformatics, genomics, and metabolomics. We develop algorithms and software to tackle some of the most challenging and interesting data intensive problems in the life sciences. Our research interests include data science, big data, pattern analysis in high-dimensionality data sets, evolutionary computation and optimization, machine learning, computational genomics, cloud computing, computational metabolomics, and eScience. We currently have multidisciplinary projects underway in metabolomics, human cognition, toxicology, marine biology, medical genomics, biomedical informatics, and marine genomics.\r\n</p>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}